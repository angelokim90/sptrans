{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddf8cc5f-f37d-4cf1-84f4-67da9cc373c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chama_spark():\n",
    "    from minio import Minio\n",
    "    from minio.commonconfig import CopySource\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import col, explode, current_timestamp, from_utc_timestamp, lit\n",
    "    import datetime\n",
    "    import pytz\n",
    "    import re\n",
    "    import psycopg2\n",
    "    import time\n",
    "\n",
    "    # === Spark Session ===\n",
    "    spark = (\n",
    "        SparkSession\n",
    "        .builder\n",
    "        .appName('pipeline_sptrans')\n",
    "        .config('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension')\n",
    "        .config('spark.sql.catalog.spark_catalog', 'org.apache.spark.sql.delta.catalog.DeltaCatalog')\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", \"datalake\")\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", \"datalake\")\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "\n",
    "    # === MinIO Client ===\n",
    "    client = Minio(\n",
    "        \"minio:9000\",\n",
    "        access_key=\"datalake\",\n",
    "        secret_key=\"datalake\",\n",
    "        secure=False\n",
    "    )\n",
    "\n",
    "    # === Datas ===\n",
    "    hoje = datetime.datetime.today()\n",
    "    d = hoje.astimezone(pytz.timezone('America/Sao_Paulo'))\n",
    "    data_str = d.strftime('%Y-%m-%d')\n",
    "    data_padrao = d.strftime('%Y%m%d')\n",
    "\n",
    "    bucket_raw = \"raw\"\n",
    "    origem_raw = f\"api/{data_str}/\"\n",
    "\n",
    "    # === PostgreSQL ===\n",
    "    postgres_url = \"jdbc:postgresql://db:5432/db\"\n",
    "    properties = {\n",
    "        \"user\": \"admin\",\n",
    "        \"password\": \"admin\",\n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    }\n",
    "    psql_conn_info = {\n",
    "        \"host\": \"db\",\n",
    "        \"port\": 5432,\n",
    "        \"database\": \"db\",\n",
    "        \"user\": \"admin\",\n",
    "        \"password\": \"admin\"\n",
    "    }\n",
    "\n",
    "    # === Espera arquivos no raw ===\n",
    "    objetos = []\n",
    "    tentativas = 0\n",
    "    max_tentativas = 10\n",
    "    espera_segundos = 10\n",
    "\n",
    "    while not objetos and tentativas < max_tentativas:\n",
    "        todos_objetos = list(client.list_objects(bucket_raw, prefix=origem_raw, recursive=False))\n",
    "        objetos = [obj for obj in todos_objetos if data_padrao in obj.object_name.split('/')[-1]]\n",
    "        \n",
    "        if objetos:\n",
    "            break\n",
    "        \n",
    "        print(f\"Nenhum arquivo {data_padrao} encontrado em {bucket_raw}/{origem_raw}. Aguardando {espera_segundos}s...\")\n",
    "        time.sleep(espera_segundos)\n",
    "        tentativas += 1\n",
    "\n",
    "    if not objetos:\n",
    "        print(f\"Nenhum arquivo chegou após {max_tentativas*espera_segundos}s. Encerrando execução.\")\n",
    "        spark.stop()\n",
    "        return\n",
    "\n",
    "    print(f\"Arquivos encontrados em {bucket_raw}/{origem_raw}:\")\n",
    "    for obj in objetos:\n",
    "        print(f\"- Nome: {obj.object_name}, Tamanho: {obj.size} bytes\")\n",
    "\n",
    "    # === Processamento de cada arquivo ===\n",
    "    for obj in objetos:\n",
    "        if obj.size and obj.size > 0 and re.match(r'^[a-zA-Z0-9]', obj.object_name.split('/')[-1]):\n",
    "            path_file = f\"s3a://{bucket_raw}/{obj.object_name}\"\n",
    "            print(f\"\\nProcessando arquivo: {path_file}\")\n",
    "\n",
    "            conn = psycopg2.connect(**psql_conn_info)\n",
    "            cur = conn.cursor()\n",
    "\n",
    "            try:\n",
    "                # === Log no processamento_arquivo ===\n",
    "                cur.execute(\"\"\"\n",
    "                    INSERT INTO db.monitoramento_log.processamento_arquivo(nm_arquivo, dt_processamento, nr_tamanho_byte)\n",
    "                    VALUES (%s, NOW() AT TIME ZONE 'America/Sao_Paulo', %s)\n",
    "                    RETURNING id\n",
    "                \"\"\", (obj.object_name, obj.size))\n",
    "                id_arquivo = cur.fetchone()[0]\n",
    "                conn.commit()\n",
    "\n",
    "                # === Leitura JSON ===\n",
    "                df_raw = spark.read.option(\"multiline\", \"true\").json(path_file)\n",
    "\n",
    "                # === Verifica vazio ===\n",
    "                arquivo_vazio = False\n",
    "                if df_raw.rdd.isEmpty():\n",
    "                    arquivo_vazio = True\n",
    "                elif df_raw.count() == 1:\n",
    "                    first_row = df_raw.head()\n",
    "                    if all(v is None for v in first_row):\n",
    "                        arquivo_vazio = True\n",
    "\n",
    "                if arquivo_vazio:\n",
    "                    msg = f\"Arquivo vazio/nulo: {obj.object_name}\"\n",
    "                    print(msg)\n",
    "\n",
    "                    cur.execute(\"\"\"\n",
    "                        INSERT INTO db.monitoramento_log.processos_log\n",
    "                            (nm_processo, dt_processamento, dt_delta, ds_status, ds_log, qtde_registro)\n",
    "                        VALUES (%s, NOW() AT TIME ZONE 'America/Sao_Paulo', %s, %s, %s, %s)\n",
    "                    \"\"\", (\"Arquivo para bronze\", data_str, 0, msg, 0))\n",
    "                    conn.commit()\n",
    "\n",
    "                    destino_arquivo = obj.object_name.replace(origem_raw, origem_raw + \"processed/\", 1)\n",
    "                    source = CopySource(bucket_raw, obj.object_name)\n",
    "                    client.copy_object(bucket_raw, destino_arquivo, source)\n",
    "                    client.remove_object(bucket_raw, obj.object_name)\n",
    "\n",
    "                    cur.close()\n",
    "                    conn.close()\n",
    "                    continue\n",
    "\n",
    "                # === Transformações ===\n",
    "                df_exploded = df_raw.select(col(\"hr\"), explode(col(\"l\")).alias(\"item\"))\n",
    "                df_final = df_exploded.select(\n",
    "                    col(\"hr\"), col(\"item.c\").alias(\"c\"), col(\"item.cl\").alias(\"cl\"),\n",
    "                    col(\"item.sl\").alias(\"sl\"), col(\"item.lt0\").alias(\"lt0\"),\n",
    "                    col(\"item.lt1\").alias(\"lt1\"), col(\"item.qv\").alias(\"qv\"),\n",
    "                    explode(col(\"item.vs\")).alias(\"vs_item\")\n",
    "                ).select(\n",
    "                    col(\"hr\"), col(\"c\"), col(\"cl\"), col(\"sl\"), col(\"lt0\"), col(\"lt1\"),\n",
    "                    col(\"qv\"), col(\"vs_item.p\").alias(\"p\"), col(\"vs_item.a\").alias(\"a\"),\n",
    "                    col(\"vs_item.ta\").alias(\"ta\"), col(\"vs_item.py\").alias(\"py\"),\n",
    "                    col(\"vs_item.px\").alias(\"px\"), col(\"vs_item.sv\").alias(\"sv\"),\n",
    "                    col(\"vs_item.is\").alias(\"vs_is\")\n",
    "                ).withColumn(\"dt_processamento\", from_utc_timestamp(current_timestamp(), \"America/Sao_Paulo\")) \\\n",
    "                 .withColumn(\"id_arquivo\", lit(id_arquivo))\n",
    "\n",
    "                # === Grava trusted ===\n",
    "                nome_arquivo_raw = obj.object_name.split('/')[-1].split('.')[0]\n",
    "                path_trusted = f\"s3a://trusted/api/{data_str}/{nome_arquivo_raw}_p.parquet\"\n",
    "                df_final.coalesce(1).write.mode(\"overwrite\").parquet(path_trusted)\n",
    "\n",
    "                # === Inserir no bronze ===\n",
    "                df_final.write.jdbc(url=postgres_url, table=\"bronze_sptrans.posicao\", mode=\"append\", properties=properties)\n",
    "\n",
    "                qtde_registro = df_final.count()\n",
    "                msg = f\"Arquivo {obj.object_name.strip()} processado com sucesso.\"\n",
    "\n",
    "                cur.execute(\"\"\"\n",
    "                    INSERT INTO db.monitoramento_log.processos_log\n",
    "                        (nm_processo, dt_processamento, dt_delta, ds_status, ds_log, qtde_registro)\n",
    "                    VALUES (%s, NOW() AT TIME ZONE 'America/Sao_Paulo', %s, %s, %s, %s)\n",
    "                \"\"\", (\"Arquivo para bronze\", data_str, 1, msg, qtde_registro))\n",
    "                conn.commit()\n",
    "\n",
    "                destino_arquivo = obj.object_name.replace(origem_raw, origem_raw + \"processed/\", 1)\n",
    "                source = CopySource(bucket_raw, obj.object_name)\n",
    "                client.copy_object(bucket_raw, destino_arquivo, source)\n",
    "                client.remove_object(bucket_raw, obj.object_name)\n",
    "\n",
    "                print(msg)\n",
    "\n",
    "            except Exception as e:\n",
    "                erro_msg = f\"Erro ao processar {obj.object_name}: {e}\"\n",
    "                print(erro_msg)\n",
    "\n",
    "                cur.execute(\"\"\"\n",
    "                    INSERT INTO db.monitoramento_log.processos_log\n",
    "                        (nm_processo, dt_processamento, dt_delta, ds_status, ds_log, qtde_registro)\n",
    "                    VALUES (%s, NOW() AT TIME ZONE 'America/Sao_Paulo', %s, %s, %s, %s)\n",
    "                \"\"\", (\"Arquivo para bronze\", data_str, 0, erro_msg, 0))\n",
    "                conn.commit()\n",
    "\n",
    "            finally:\n",
    "                cur.close()\n",
    "                conn.close()\n",
    "\n",
    "    # === Finaliza Spark ===\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645cd9e0-e16e-422a-b542-f3e6d6c667ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
